{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56982f53",
   "metadata": {},
   "source": [
    "This is a complete NLP pipeline for Meta's earnings call sentiment extraction with LLMs, without the needs of human-labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8ec70",
   "metadata": {},
   "source": [
    "**1. Data Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509586f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization with spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    # strip to avoid leading/trailing blanks\n",
    "    doc = nlp(text.strip())\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3 speaker patterns\n",
    "speaker_patterns = [\n",
    "    # 1. \"Name, Title\"\n",
    "    re.compile(\n",
    "        r'^(?=[A-Z][A-Za-z0-9 ]{3,20},\\s*[A-Z][A-Za-z ]{1,20}$)'\n",
    "        r'(?P<speaker>[A-Z][A-Za-z0-9 ]+,\\s*[A-Z][A-Za-z ]+)\\s*$',\n",
    "        re.MULTILINE\n",
    "    ),\n",
    "\n",
    "    # 2. \"Name:\"\n",
    "    re.compile(\n",
    "        r'^(?=[A-Z][A-Za-z0-9 ]{1,20}:)'\n",
    "        r'(?P<speaker>(?:[A-Z]{2,}|[A-Z][a-z]+)'\n",
    "        r'(?:\\s+(?:[A-Z]{2,}|[A-Z][a-z]+)){0,4}):',\n",
    "        re.MULTILINE\n",
    "    ),\n",
    "\n",
    "    # 3. \"Name, Title, Subtitle\"\n",
    "    re.compile(\n",
    "        r'^(?P<speaker>'                                \n",
    "        r'[A-Z][A-Za-z0-9]+(?:\\s+[A-Z][A-Za-z0-9]+)*'   \n",
    "        r'(?:'                                          \n",
    "          r',\\s*[A-Z][A-Za-z]+(?:\\s+[A-Za-z]+)*'        \n",
    "        r'){2,}'                                        \n",
    "        r')\\s*$',                                       \n",
    "        re.MULTILINE\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speaker_matches(text):\n",
    "    matches = []\n",
    "    for pat in speaker_patterns:\n",
    "        matches.extend(pat.finditer(text))\n",
    "    return sorted(matches, key=lambda m: m.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5753a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quarter(filename: str) -> str:\n",
    "    m = re.search(r'-Q([1-4])-(\\d{4})-', filename)\n",
    "    if m:\n",
    "        quarter, year = m.group(1), m.group(2)\n",
    "        return f\"{year}Q{quarter}\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(speaker: str) -> str:\n",
    "    base = speaker.strip().rstrip(':').strip()\n",
    "    return base.split(',', 1)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbf4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_position(name: str) -> str:\n",
    "    position_map = {\n",
    "        \"Mark Zuckerberg\": \"CEO\",\n",
    "        \"Susan Li\": \"CFO\",\n",
    "        \"Kenneth Dorell\": \"Investor Relations Director\",\n",
    "        \"Operator\": \"Operator\",\n",
    "    }\n",
    "    return position_map.get(name, \"Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b10bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove page numbers from text\n",
    "def remove_page_numbers(page_text):\n",
    "    lines = page_text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if not re.match(r'^\\s*(Page\\s*)?\\d+\\s*$', line):\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript(pdf_path):\n",
    "    output = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Remove page numbers from each page\n",
    "        text = \"\\n\".join(\n",
    "            remove_page_numbers(page.extract_text() or \"\") for page in pdf.pages\n",
    "        )\n",
    "\n",
    "    matches = find_speaker_matches(text)\n",
    "    for idx, m in enumerate(matches):\n",
    "        raw_speaker = m.group('speaker').strip()\n",
    "        start = m.end()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)\n",
    "        block_text = text[start:end].replace(\"\\n\", \" \").strip()\n",
    "        sentences = [s for s in split_sentences(block_text) if s and len(s.split()) > 7]\n",
    "        output.append({\n",
    "            \"raw_speaker\": raw_speaker,\n",
    "            \"sentences\": sentences,\n",
    "        })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, output_json_path):\n",
    "    all_records = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        quarter = extract_quarter(filename)\n",
    "        for block in extract_transcript(pdf_path):\n",
    "            raw = block[\"raw_speaker\"]\n",
    "            name = extract_name(raw)\n",
    "            pos  = extract_position(name)\n",
    "            all_records.append({\n",
    "                \"filename\": filename,\n",
    "                \"quarter\": quarter,\n",
    "                \"speaker\": name,       \n",
    "                \"position\": pos,\n",
    "                \"sentences\": block[\"sentences\"],\n",
    "            })\n",
    "\n",
    "    print(f\"Saving {len(all_records)} records to {output_json_path}...\")\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_records, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2918398",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder = \"Earnings Call Transcript\"\n",
    "    output_file = \"Earnings Call Transcript.json\"\n",
    "    process_folder(folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Earnings Call Transcript.json\")\n",
    "df = df.explode(\"sentences\").rename(columns={\"sentences\": \"sentence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = range(1, len(df) + 1)\n",
    "df.drop('filename', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09283729",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id'] + [c for c in df.columns if c != 'id']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e496b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_marker = [\"With that, Krista, letâ€™s open up the call for questions.\"]\n",
    "mask = df['sentence'].apply(lambda s: any(marker in s for marker in qna_marker))\n",
    "qna_sentences = df.loc[mask,['quarter', 'speaker', 'sentence']]\n",
    "print(qna_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_id = qna_sentences.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all as Presentation first\n",
    "df['section'] = 'Presentation'\n",
    "\n",
    "# Dictionary mapping quarters to Q&A start indices\n",
    "qna_start_indices = {\n",
    "    '2024Q1': qna_id[0],\n",
    "    '2025Q1': qna_id[1],\n",
    "    '2024Q2': qna_id[2],\n",
    "    '2024Q3': qna_id[3],\n",
    "    '2024Q4': qna_id[4],\n",
    "}\n",
    "\n",
    "for quarter, start_idx in qna_start_indices.items():\n",
    "    mask = (df['quarter'] == quarter) & (df.index > start_idx)\n",
    "    df.loc[mask, 'section'] = 'Q&A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95511953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['section'] == 'Q&A'][['quarter','speaker','position','sentence']]\n",
    "df1[df1['quarter']  == '2025Q1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c5595",
   "metadata": {},
   "source": [
    "**2.Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eadca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text-extracted dataset\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70da5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def data_preprocess_pipeline_llm(text):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Replace multiple dashes with a space\n",
    "    text = re.sub(r'--+', ' ', text)\n",
    "\n",
    "    # Run spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "        \n",
    "    # Keep all tokens except spaces\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_space]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data-preprocessing pipeline\n",
    "df['sentence'] = df['sentence'].apply(data_preprocess_pipeline_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Q&A section only\n",
    "df1 = df[df['section'] == 'Q&A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"df1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b39ce2",
   "metadata": {},
   "source": [
    "**3. Sentiment classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192be88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df1[~df1['speaker'].isin(['Operator', 'Kenneth Dorell'])]\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['id','quarter','sentence']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b376b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data['quarter'].isin(['2024Q1', '2024Q2', '2024Q3'])]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.drop(train_data.index)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d896d",
   "metadata": {},
   "source": [
    "**ZERO-SHOT PROMPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred_label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492095cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57017168",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"sk-58c0df73519c42debe27d41e164d455a\", base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "batch_size = 10\n",
    "\n",
    "for i in range(0,len(test_data),batch_size):\n",
    "    batches.append(test_data[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(batch,current_batch,total_batch, model='deepseek-chat'):\n",
    "    print(f\"Processing batch {current_batch+1} of {total_batch}\")\n",
    "    \n",
    "    json_data = batch[['sentence','pred_label']].to_json(orient='records')\n",
    "    \n",
    "    zero_shot_prompt = f\"\"\" You are an advanced sentiment analysis assistant. \n",
    "    Your task is to classify sentiment and give sentiment score for each sentence extracted from an earnings call transcript as -1 for negative sentiment, 0 for neutral sentiment, 1 for positive sentiment. \n",
    "    The sentiment score should be an integer. \n",
    "    The purpose is to extract the trading sentiment so as to have an edge in after-market trading. \n",
    "    The sentences are provided between three backticks below.\n",
    "    Return **only** a valid JSON code as output - which is provided between three backticks.\n",
    "    Update the predicted sentiment score under the 'pred_label' in the JSON code.\n",
    "    Do not make any changes to the JSON format.\n",
    "    \n",
    "    ```\n",
    "    {json_data}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    print(zero_shot_prompt)\n",
    "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c445bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = len(batches)\n",
    "responses = []\n",
    "\n",
    "for i in range(0,len(batches)):\n",
    "    response = get_completion(batches[i], i, batch_count)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "df_total0 = pd.DataFrame()\n",
    "\n",
    "for response in responses: \n",
    "    cleaned = re.sub(r\"^```(json)?|```$\", \"\", response.strip()).strip()\n",
    "    data = json.loads(cleaned)\n",
    "    df_temp = pd.DataFrame(data)\n",
    "    df_total0 = pd.concat([df_total0, df_temp], ignore_index=True)\n",
    "\n",
    "    \n",
    "print(df_total0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred_label'] = df_total0['pred_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
